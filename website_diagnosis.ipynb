{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<a href=\"https://jy2014.github.io/EpilepsyPrediction/Home.html\" target=\"_self\">Back to the Home Page</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<a href=\"https://jy2014.github.io/EpilepsyPrediction/Home.html\" target=\"_self\">Back to the Home Page</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Predicting Epilepsy Status #\n",
    "\n",
    "_________________________\n",
    "<font size=\"3\"><p>After the missing values were imputed, models were built to predict epilepsy status using the survey data. Multiple classification models were tuned, including Linear Discriminant Analysis (LDA), Quadratic Discriminant Anlysis (QDA), Weighted Logistic Regression and Random Forest. Due to that the dataset is highly imbalanced with the ratio of epilepsy to non-epilepsy respondents of 1 : 100, we tuned the class weight or priors in addition to regularization parameters, and determined to use F1 score instead of the overall accuracy rate to evaluate our models.</p>\n",
    "<p></p>\n",
    "The model selection procedure and results are shown in details below. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression as Log\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '3'>\n",
    "- [4.1. Read the imputed dataset](#4.1.Read-the-imputed-dataset)\n",
    "- [4.2. Model Selection](#4.2.Model-Selection)\n",
    "- [4.3. Model Comparison](#4.3.Model-Comparison)\n",
    "- [4.4. Feature Importance](#4.4.Feature-Importance)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** please click the button on top of the page to view the code ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 4.1.Read the imputed dataset ##\n",
    "<p></p>\n",
    "<font size = \"3\">The imputed data set saved from the previous chapter was read in. The respondents who did not provide information to the epilepsy questions were removed from the analysis, and one-hot encoding was applied to categorical variables.</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read dataset with imputated dataset (Random_Forest)\n",
    "df_final = pd.read_csv('imputed_PovertyLevel_RF.csv')\n",
    "\n",
    "# Drop the first two columns as well as ID number, as it is of no relevance\n",
    "df_final = df_final.drop(df_final.columns[[0, 1, 2]], axis=1)\n",
    "# remove the respondents missing epilepsy information\n",
    "df_final = df_final[df_final.K2Q42A != 6]\n",
    "df_final = df_final[df_final.K2Q42A != 7]\n",
    "# extract epilepsy status as y\n",
    "df_y = df_final['K2Q42A']\n",
    "# remove other epilepsy related info from the predictors\n",
    "df_x = df_final.drop(['K2Q42A','K2Q42B','K2Q42C'], 1)\n",
    "\n",
    "# Read categorical dataset, and then apply one-hot encoder\n",
    "df_cat = pd.read_csv('Categorical_Column_Names_wState.csv', header = None)\n",
    "# Read categorical dataset\n",
    "cat = df_cat.iloc[:, 1]\n",
    "cat = np.array(cat)\n",
    "categorical_names = cat\n",
    "\n",
    "# remove epilepsy related names\n",
    "categorical_names = categorical_names[categorical_names != 'K2Q42A']\n",
    "categorical_names = categorical_names[categorical_names != 'K2Q42B']\n",
    "categorical_names = categorical_names[categorical_names != 'K2Q42C']\n",
    "\n",
    "# Apply one hot endcoing\n",
    "DF_for_impute_dummies = pd.get_dummies(df_x, columns = categorical_names)\n",
    "\n",
    "# reorganize the variables\n",
    "df_x = DF_for_impute_dummies\n",
    "df_y = df_y\n",
    "df_x_array = np.array(df_x)\n",
    "df_y_array = np.array(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 4.2.Model Selection ##\n",
    "<p></p>\n",
    "<font size = \"3\"><p>As our goal is to accurately predict epilepsy status for diagnosis purpose, we need to consider both true positive and true negative. Therefore, instead of just taking the overall accuracy rate (true positive) into account, we evaluated our models based on F1 score, which combines the information of both precision and recall. </p>\n",
    "<p></p>\n",
    "$$ F1 = \\frac{2 \\times Precison \\times Recall}{Precision + Recall} $$\n",
    "\n",
    "<p>After fitting several models using cross validation, we found out that weighted logistic regression ( C = 0.001, Class_weight = {0:1, 1:10.0} ) would give us the highest F1 score, which reached around 0.26. Moreover, the accuracy rate was also very high, whcih was around 0.98. Considering that this is a survey dataset with thousands of features, we were satisfied with the model we got.</p></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = \"4\"> <b>Baseline Models</b></font>\n",
    "<p></p>\n",
    "    <font size = \"3\"><p>If observations are randomly assigned to the epilepsy or non-epilepsy groups, the overall accuracy rate is 0.5, and the F1 score 0.02. If all the observations are classified into the non-epilepsy group, which is a non-weighted logisctic regression model does, the overall accuracy rate is 0.99. However, the F1 score is almost zero. The result is consistent with the fact that the classes are highly imbalance in this dataset. Given our goal of detecting epilepsy patients, F1-score is a better measure than the overall accuracy rate. </p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### randomly assign classes\n",
    "y_pred = np.random.binomial(1, 0.5, 90539)\n",
    "\n",
    "## accuracy\n",
    "# print \"accuracy: \", np.mean(y_pred == df_y_array)\n",
    "# print \"F1:\", metrics.f1_score(df_y_array, y_pred)\n",
    "# print \"precision: \", metrics.precision_score(df_y_array, y_pred)\n",
    "# print \"recall: \", metrics.recall_score(df_y_array, y_pred)\n",
    "\n",
    "### simple non-weighted logistic regression\n",
    "y_pred = np.loadtxt(\"y_pred_simplelog.txt\", delimiter = ',').reshape((90539, 1))\n",
    "\n",
    "## accuracy\n",
    "# print \"accuracy: \", np.mean(y_pred == df_y_array)\n",
    "# print \"F1:\", metrics.f1_score(df_y_array, y_pred)\n",
    "# print \"precision: \", metrics.precision_score(df_y_array, y_pred)\n",
    "# print \"recall: \", metrics.recall_score(df_y_array, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = \"4\"> <b>Linear Discriminant Analysis (LDA) </b></font>\n",
    "<p></p>\n",
    "    <font size = \"3\"><p>We performed Linear Discriminant Analysis (LDA) using five-fold cross-validation. The priors were tuned to accommodate the class imbalance. The best model were chosen based on the F1 score, which generated an overall accuracy rate of 0.98, F1 score of 0.25. </p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA\n",
    "model = LDA()\n",
    "\n",
    "# five fold stratified CV\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "# tune priors\n",
    "weights = np.concatenate((np.arange(2,10,1), np.logspace(1,5,5)), axis = 0)\n",
    "prior_list = [np.asarray([1-1/weights[i], 1/weights[i]]) for i in range(len(weights))]\n",
    "\n",
    "# tune the model\n",
    "grid_model = GridSearchCV(model, param_grid = {'priors': prior_list}, cv  = skf, scoring = 'f1')\n",
    "grid_model.fit(df_x_array, df_y_array)\n",
    "lda = grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### parameters of the best model were recorded for future use\n",
    "# because runing the model selection code again will TAKE A LONG TIME\n",
    "###\n",
    "\n",
    "# the resulted best model\n",
    "lda = LDA(priors=np.asarray([9.99900e-01,   1.00000e-04]), shrinkage=None,\n",
    "              solver='svd', store_covariance=False, tol=0.0001)\n",
    "\n",
    "# predict using CV predict\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "y_pred = cross_val_predict(lda, df_x, df_y, cv = skf) \n",
    "\n",
    "# print metrics.f1_score(df_y, y_pred)\n",
    "# metrics.confusion_matrix(df_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = \"4\"> <b>Quadratic Discriminant Analysis (QDA)</b></font>\n",
    "<p></p>\n",
    "    <font size = \"3\"><p>We performed Quadratic Discriminant Analysis (QDA) using five-fold cross-validation. The regularization parameters and priors were tuned to accommodate the class imbalance and large number of predictors. The best model was selected based on F1 scores. The best QDA model generated an accuracy rate of 0.12 and F1 score of 0.02. The low accuracy and F1 score is due to a high recall of 0.93.</p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QDA\n",
    "model = QDA()\n",
    "\n",
    "# five fold stratified CV\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "# tune priors\n",
    "weights = np.concatenate((np.arange(2,10,2), np.logspace(1,5,5)), axis = 0)\n",
    "prior_list = [np.asarray([1-1/weights[i], 1/weights[i]]) for i in range(len(weights))]\n",
    "# tune regularization parameter\n",
    "C = range(-4, 5, 1)\n",
    "c = np.power(10.0, C)\n",
    "\n",
    "# tune the model\n",
    "grid_model = GridSearchCV(model, param_grid = {'priors': prior_list, 'reg_param': c},\n",
    "                          cv  = skf, scoring = 'f1')\n",
    "grid_model.fit(df_x_array, df_y_array)\n",
    "qda = grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### parameters of the best model were recorded for future use\n",
    "# because runing the model selection code again will TAKE A LONG TIME\n",
    "###\n",
    "\n",
    "# the resulted best model\n",
    "qda = QDA(priors=[  9.99990e-01,   1.00000e-05],\n",
    "               reg_param=1.0, store_covariances=False, tol=0.0001)\n",
    "# predict using CV predict\n",
    "y_pred = cross_val_predict(qda, df_x, df_y, cv = skf) \n",
    "\n",
    "# print metrics.f1_score(df_y, y_pred)\n",
    "# metrics.confusion_matrix(df_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = \"4\"> <b> Logistic regression (with subsampling)</b></font>\n",
    "<p></p>\n",
    "    <font size = \"3\"><p>We performed Quadratic Discriminant Analysis (QDA) using five-fold cross-validation. We tuned several parameters for logistic regression, including the regularization parameter $C$, class weights and penalty types (L1 or L2). Furthermore, we encountered the problem of high computational requirement during the process of fitting logistic regression to the dataset, because the dataset includes more than 90,000 entries and 1,500 features. In order to allocate our time and computational capacity more efficiently, we randomly selected a subsample of the non-epilepsy group to use in the analysis. We tested the subsample size of different ratios of non-epilepsy to epilepsy respondents, including 10:1 and 20:1. The subsample was combined with the epilepsy group for the model selection. The optimal values of the parameters were selected based on five-fold cross-validation, and the model was evaluated on the entire dataset. The best logistic regression model generates an F1 score of 0.26 and an overall accuracy of 0.97.</p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Because performing Logistic regression on the entire dataset TAKES A LONG TIME\n",
    "# subsets of the healthy people were sampled and combined with the epi patients \n",
    "# the ratio of health to epi is 20:1\n",
    "# models were selected using this subsampled dataset\n",
    "###\n",
    "\n",
    "random shuffle the rows\n",
    "n = df_x_array.shape[0]\n",
    "perm = range(n)\n",
    "np.random.shuffle(perm)\n",
    "\n",
    "x = df_x_array[perm]\n",
    "y = df_y_array[perm]\n",
    "\n",
    "# Resubsample 20:1 first\n",
    "# extract epilepsy patients data\n",
    "x_epi = x[y==1]\n",
    "y_epi = y[y==1]\n",
    "# healthy patients data\n",
    "x_health = x[y==0]\n",
    "y_health = y[y==0]\n",
    "\n",
    "# sample 19340 healthy patients (if healthy : epilepsy = 20 : 1)\n",
    "# sample 9670 healthy patients (if healthy : epilepsy = 10 : 1)\n",
    "# the data has already been shuffled\n",
    "x_health = x_health[:19341]\n",
    "y_health = y_health[:19341]\n",
    "\n",
    "# combine the epilepsy and sampled healthy data\n",
    "x2 = np.concatenate((x_epi, x_health), axis = 0)\n",
    "y2 = np.concatenate((y_epi, y_health), axis = 0)\n",
    "\n",
    "# shuffle the combined data\n",
    "n3 = x2.shape[0]\n",
    "perm2 = range(n3)\n",
    "np.random.shuffle(perm2)\n",
    "\n",
    "x2 = x2[perm2]\n",
    "y2 = y2[perm2]\n",
    "\n",
    "# CV to tune the parameters, including C, 'l2/l1', Class_weight\n",
    "model = Log()\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "C = range(-4, 5, 1)\n",
    "c = np.power(10.0, C)\n",
    "weights = np.logspace(1,5,5)\n",
    "weight_list_dict = [{0:1, 1: weights[i]} for i in range(len(weights))]\n",
    "grid_model = GridSearchCV(model, param_grid = {'C': c, 'class_weight': weight_list_dict}, \n",
    "                          cv  = skf, scoring = 'f1')\n",
    "grid_model.fit(x2, y2)\n",
    "log = grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### parameters of the best model were recorded for future use\n",
    "# because runing the model selection code again will TAKE A LONG TIME\n",
    "###\n",
    "\n",
    "# the resulted best model\n",
    "log = Log(C = 0.001, class_weight = {0:1, 1:10})\n",
    "\n",
    "y_pred = cross_val_predict(log, df_x, df_y, cv = skf) \n",
    "\n",
    "# print metrics.f1_score(df_y, y_pred)\n",
    "# metrics.confusion_matrix(df_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = \"4\"> <b>Random forest</b></font>\n",
    "<p></p>\n",
    "    <font size = \"3\"><p>We also performed random forest using five-fold cross-validation. The tuned parameters include 'number of trees', 'class weights' and 'max_depth', and the best model was selected based on F1 scores. The best Random Forest model had an overall accuracy rate of 0.98, with F1 score to be 0.19. The F1 score was lower compared to the best weighted logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold (n_splits = 3, shuffle = True)\n",
    "\n",
    "model = ensemble.RandomForestClassifier(n_estimators = 10)\n",
    "#n_trees = np.arange(10, 100, 20)  # Trees and depth are explored on an exponentially growing space,\n",
    "depths = np.arange(10, 30, 5).astype(int)   # since it is assumed that trees and depth will add accuracy in a decaying fashion.\n",
    "weights = np.logspace(1,4,4).astype(int)\n",
    "weight_list_dict = [{0:int(1), 1: weights[i]} for i in range(len(weights))]\n",
    "\n",
    "grid_model = GridSearchCV(model , n_jobs = 5, param_grid = {'class_weight': weight_list_dict, 'max_depth': depths}, \n",
    "                          cv  = skf, scoring = 'f1')\n",
    "grid_model.fit(df_x_drop, df_y_drop)\n",
    "best_forest = grid_model.best_estimator_\n",
    "\n",
    "#print best_forest\n",
    "\n",
    "best_forest_description = str(best_forest.get_params)\n",
    "f = open ('RandomForest_best_params'+'.p', 'wb')\n",
    "pickle.dump([best_forest_description], f)\n",
    "f.close()\n",
    "\n",
    "y_pred = cross_val_predict(best_forest, df_x_drop, df_y_drop, cv = skf)\n",
    "f = open ('RandomForest_y_pred'+'.p', 'wb')\n",
    "pickle.dump([y_pred], f)\n",
    "f.close()\n",
    "\n",
    "# print 'F1 score CV:', f1_score (df_y_drop, y_pred)\n",
    "\n",
    "# print 'F1 score CV: ', cross_val_score(best_forest, df_x_drop, df_y_drop, cv=skf, scoring = 'f1')\n",
    "# print 'R2 score CV: ', cross_val_score(best_forest, df_x_drop, df_y_drop, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### parameters of the best model were recorded for future use\n",
    "# because runing the model selection code again will take a long time\n",
    "###\n",
    "\n",
    "# the resulted best model\n",
    "rf = ensemble.RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 100},\n",
    "                                     criterion='gini', max_depth=15, max_features='auto')\n",
    "\n",
    "y_pred = cross_val_predict(log, df_x, df_y, cv = skf) \n",
    "\n",
    "# print metrics.f1_score(df_y, y_pred)\n",
    "# metrics.confusion_matrix(df_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.Model Comparison ###\n",
    "\n",
    "<font size=\"3\">After the best parameters were selected for Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Logistic Regression and Random Forest. The results from all the models were compared, and the best model was selected based on the highest F1 score. The regularized weighted logistic regression provided the highest F1 score of 0.26 as well as a comparabily high accuracy rate of 0.97. Therefore, <b>the regularized weighted logistic regression was selected to be the best model for the prediction of epilepsy diagnosis.</b>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='pic/Diagnosis_1.jpg' alt=\"table of model results\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table of model results](pic/Diagnosis_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=\"3\"><b> ROC plot of the models</b></font></p>\n",
    "<p><font size=\"3\"> Receiver Operating Characteristic (ROC) curves of the models were plotted. ROC is a graphical plot that illustrates the performance of a classifier system as the discrimination threshold varies. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold values. The ROC curve hugging the upper left corner suggests that the model performs well on the classification. </p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ROC_plot (y_test, y_pred, col, name, ax):\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    ax.plot(false_positive_rate, true_positive_rate, color = col, label= name + ':AUC = %0.2f'% roc_auc)\n",
    "    ax.plot([0,1],[0,1],'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the best models\n",
    "lda = LDA(priors=np.asarray([9.99900e-01,   1.00000e-04]), shrinkage=None,\n",
    "              solver='svd', store_covariance=False, tol=0.0001)\n",
    "qda = QDA(priors=[  9.99990e-01,   1.00000e-05],\n",
    "               reg_param=1.0, store_covariances=False, tol=0.0001)\n",
    "log = Log(C = 0.001, class_weight = {0:1, 1:10})\n",
    "\n",
    "rf = ensemble.RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 100},\n",
    "                                     criterion='gini', max_depth=15, max_features='auto')\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x_array, df_y_array, test_size=0.3)\n",
    "\n",
    "# train each model on the training set\n",
    "lda.fit(x_train, y_train)\n",
    "qda.fit(x_train, y_train)\n",
    "log.fit(x_train, y_train)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# extract the probability on the testing set\n",
    "proba_log = log.predict_proba(x_test)[:, 1]\n",
    "proba_lda = lda.predict_proba(x_test)[:, 1]\n",
    "proba_qda = qda.predict_proba(x_test)[:, 1]\n",
    "proba_rf = rf.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot ROC of the models\n",
    "color = ['orange','g', 'b','deepskyblue']\n",
    "names = ['LogReg', 'LDA', 'QDA', 'RandomForest']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (6, 6))\n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "ROC_plot(y_test, proba_log, color[0], names[0], ax)\n",
    "ROC_plot(y_test, proba_lda, color[1], names[1], ax)\n",
    "ROC_plot(y_test, proba_qda, color[2], names[2], ax)\n",
    "ROC_plot(y_test, proba_rf, color[3], names[3], ax)\n",
    "    \n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate', fontsize = 16)\n",
    "plt.xlabel('False Positive Rate', fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "# the output was suppressed because it was not centered\n",
    "# a jpeg figure is inserted instead to make it look better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='pic/Diagnosis_2.jpg' alt=\"ROC curves\" width=\"450\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4.Feature Importance ###\n",
    "<p><font size=\"3\"> Based on the comparison above, we decided to choose the weighted logistic regression model to indentify risk factors associated with epilepsy. Higher absolute value of the coefficient of a feature suggests that the feature is more associated with the response variable. However, the top-ranked features in the list are probably caused by epilepsy instead of being able to predict epilepsy, which also reflects the limitation of our dataset as a single-point observation. Therefore, it would limit the prediction function of our model in real world status.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p><font size=\"3\"><b> The table summarizes the 10 questions with the highest importance in the model.</b></font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Rank          | Question Topic           \n",
    "| ------------- |:-------------\n",
    "| 1             | Current Developmental Delay\n",
    "| 2             | Ever Diagnosed Developmental Delay\n",
    "| 3             | Learning Disability\n",
    "| 4             | Ever Diagnosed Developmental Delay\n",
    "| 5             | Individualized Education Program\n",
    "| 6             | Learning Disability\n",
    "| 7             | Individualized Education Program\n",
    "| 8             | Developmental Delay Severity\n",
    "| 9             | Current Developmental Delay\n",
    "| 10            | Learning Disability Severity\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.fit(df_x_array, df_y_array)\n",
    "coef = log.coef_\n",
    "coef_list = coef[0]\n",
    "\n",
    "# As we want to compare the absolute value of coefficients here, we need to change negative ones to be positive\n",
    "coef_change = []\n",
    "for i in range(len(coef_list)):\n",
    "    if coef_list[i] < 0:\n",
    "        coef_change.append(-coef_list[i])\n",
    "    else:\n",
    "        coef_change.append(coef_list[i])\n",
    "# Get the index from large to small\n",
    "a = np.array(coef_change)\n",
    "index = np.argsort(-a)\n",
    "# Pick the top 50 highest index, to see what the features are the most important\n",
    "index_top_50 = index[:50]\n",
    "# Now go back to find what the corresponding highest-rank features are.\n",
    "features_names = DF_for_impute_dummies.columns.values\n",
    "features_names_top_50 = features_names[index_top_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"jy2014.github.io/EpilepsyPrediction/Imputation.html\" target=\"_self\">Chapter 3. Data Imputation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"jy2014.github.io/EpilepsyPrediction/Imputation.html\" target=\"_self\">Chapter 3. Data Imputation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"jy2014.github.io/EpilepsyPrediction/QualityOfLife.html\" target=\"_self\">Chapter 5. Quality of Life</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"jy2014.github.io/EpilepsyPrediction/QualityOfLife.html\" target=\"_self\">Chapter 5. Quality of Life</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://jy2014.github.io/EpilepsyPrediction/Home.html\" target=\"_self\">Back to the Home Page</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"https://jy2014.github.io/EpilepsyPrediction/Home.html\" target=\"_self\">Back to the Home Page</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
